{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sdne\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "\n",
    "host = 'bolt://localhost:7687'\n",
    "user = 'neo4j'\n",
    "password = 'wowhi223'\n",
    "driver = GraphDatabase.driver(host,auth=(user, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def process_nxgraph(graph):\n",
    "    node2idx = {}\n",
    "    idx2node = []\n",
    "    node_size = 0\n",
    "    for node in graph.nodes():\n",
    "        node2idx[node] = node_size\n",
    "        idx2node.append(node)\n",
    "        node_size += 1\n",
    "    return idx2node, node2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regularization(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model, gamma=0.01, p=2, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        if gamma <= 0:\n",
    "            print(\"param weight_decay can not be <= 0\")\n",
    "            exit(0)\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.p = p\n",
    "        self.device = device\n",
    "        self.weight_list = self.get_weight_list(model)\n",
    "        self.weight_info = self.get_weight_info(self.weight_list)\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def forward(self, model):\n",
    "        self.weight_list = self.get_weight_list(model)\n",
    "        reg_loss = self.regulation_loss(self.weight_list, self.gamma, self.p)\n",
    "        return reg_loss\n",
    "\n",
    "    def regulation_loss(self, weight_list, gamma, p=2):\n",
    "        reg_loss = 0\n",
    "        for name, w in weight_list:\n",
    "            l2_reg = torch.norm(w, p=p)\n",
    "            reg_loss += l2_reg\n",
    "        reg_loss = reg_loss * gamma\n",
    "        return reg_loss\n",
    "\n",
    "    def get_weight_list(self, model):\n",
    "        weight_list = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                weight = (name, param)\n",
    "                weight_list.append(weight)\n",
    "        return weight_list\n",
    "\n",
    "    def get_weight_info(self, weight_list):\n",
    "        print(\"#\"*10, \"regulations weight\", \"#\"*10)\n",
    "        for name, param in weight_list:\n",
    "            print(name)\n",
    "        print(\"#\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphBaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return np.asarray(all_labels)\n",
    "\n",
    "class MultiClassifier(object):\n",
    "    def __init__(self, embeddings, clf):\n",
    "        self.embeddings = embeddings\n",
    "        self.clf = TopKRanker(clf)\n",
    "        self.binarizer = MultiLabelBinarizer()\n",
    "\n",
    "    def fit(self, X, y, y_all):\n",
    "        self.binarizer.fit(y_all)\n",
    "        X_train = [self.embeddings[x] for x in X]\n",
    "        y_train = self.binarizer.transform(y)\n",
    "        self.clf.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X, top_k_list):\n",
    "        X_ = np.asarray([self.embeddings[x] for x in X])\n",
    "        y_pred = self.clf.predict(X_, top_k_list=top_k_list)\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        top_k_list = [len(l) for l in y]\n",
    "        y_pred = self.predict(X, top_k_list)\n",
    "        y = self.binarizer.transform(y)\n",
    "        averages = [\"micro\", \"macro\", \"samples\", \"weighted\"]\n",
    "        results = {}\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(y, y_pred, average=average)\n",
    "        results['acc'] = accuracy_score(y, y_pred)\n",
    "        print('-------------------')\n",
    "        print(results)\n",
    "        print('-------------------')\n",
    "        return results\n",
    "\n",
    "    def evaluate_hold_out(self, X, y, test_size=0.2, random_state=123):\n",
    "        np.random.seed(random_state)\n",
    "        train_size = int((1-test_size) * len(X))\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(X)))\n",
    "        X_train = [X[shuffle_indices[i]] for i in range(train_size)]\n",
    "        y_train = [y[shuffle_indices[i]] for i in range(train_size)]\n",
    "        X_test = [X[shuffle_indices[i]] for i in range(train_size, len(X))]\n",
    "        y_test = [y[shuffle_indices[i]] for i in range(train_size, len(X))]\n",
    "\n",
    "        self.fit(X_train, y_train, y)\n",
    "\n",
    "        return self.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from basemodel import GraphBaseModel\n",
    "from utils import process_nxgraph\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from utils import Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDNEModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_layers, alpha, beta, device=\"cpu\"):\n",
    "        super(SDNEModel, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.device = device\n",
    "        input_dim_copy = input_dim\n",
    "        layers = []\n",
    "        for layer_dim in hidden_layers:\n",
    "            layers.append(torch.nn.Linear(input_dim, layer_dim))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            input_dim = layer_dim\n",
    "        self.encoder = torch.nn.Sequential(*layers)\n",
    "\n",
    "        layers = []\n",
    "        for layer_dim in reversed(hidden_layers[:-1]):\n",
    "            layers.append(torch.nn.Linear(input_dim, layer_dim))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            input_dim = layer_dim\n",
    "        layers.append(torch.nn.Linear(input_dim, input_dim_copy))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        self.decoder = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, A, L):\n",
    "        Y = self.encoder(A)\n",
    "        A_hat = self.decoder(Y)\n",
    "        beta_matrix = torch.ones_like(A)\n",
    "        mask = A != 0\n",
    "        beta_matrix[mask] = self.beta\n",
    "        loss_2nd = torch.mean(torch.sum(torch.pow((A - A_hat) * beta_matrix, 2), dim=1))\n",
    "        loss_1st =  self.alpha * 2 * torch.trace(torch.matmul(torch.matmul(Y.transpose(0,1), L), Y))\n",
    "        return loss_2nd + loss_1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDNE(GraphBaseModel):\n",
    "    \n",
    "    def __init__(self, graph, hidden_layers=None, alpha=1e-5, beta=5, gamma=1e-5, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.graph = graph\n",
    "        self.idx2node, self.node2idx = process_nxgraph(graph)\n",
    "        self.node_size = graph.number_of_nodes()\n",
    "        self.edge_size = graph.number_of_edges()\n",
    "        self.sdne = SDNEModel(self.node_size, hidden_layers, alpha, beta)\n",
    "        self.device = device\n",
    "        self.embeddings = {}\n",
    "        self.gamma = gamma\n",
    "\n",
    "        adjacency_matrix, laplace_matrix = self.__create_adjacency_laplace_matrix()\n",
    "        self.adjacency_matrix = torch.from_numpy(adjacency_matrix.toarray()).float().to(self.device)\n",
    "        self.laplace_matrix = torch.from_numpy(laplace_matrix.toarray()).float().to(self.device)\n",
    "\n",
    "    def fit(self, batch_size=512, epochs=1, initial_epoch=0, verbose=1):\n",
    "        num_samples = self.node_size\n",
    "        self.sdne.to(self.device)\n",
    "        optimizer = torch.optim.Adam(self.sdne.parameters())\n",
    "        if self.gamma:\n",
    "            regularization = Regularization(self.sdne, gamma=self.gamma)\n",
    "        if batch_size >= self.node_size:\n",
    "            batch_size = self.node_size\n",
    "            print('batch_size({0}) > node_size({1}),set batch_size = {1}'.format(\n",
    "                batch_size, self.node_size))\n",
    "            for epoch in range(initial_epoch, epochs):\n",
    "                loss_epoch = 0\n",
    "                optimizer.zero_grad()\n",
    "                loss = self.sdne(self.adjacency_matrix, self.laplace_matrix)\n",
    "                if self.gamma:\n",
    "                    reg_loss = regularization(self.sdne)\n",
    "                    # print(\"reg_loss:\", reg_loss.item(), reg_loss.requires_grad)\n",
    "                    loss = loss + reg_loss\n",
    "                loss_epoch += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if verbose > 0:\n",
    "                    print('Epoch {0}, loss {1} . >>> Epoch {2}/{3}'.format(epoch + 1, round(loss_epoch / num_samples, 4), epoch+1, epochs))\n",
    "        else:\n",
    "            steps_per_epoch = (self.node_size - 1) // batch_size + 1\n",
    "            for epoch in range(initial_epoch, epochs):\n",
    "                loss_epoch = 0\n",
    "                for i in range(steps_per_epoch):\n",
    "                    idx = np.arange(i * batch_size, min((i+1) * batch_size, self.node_size))\n",
    "                    A_train = self.adjacency_matrix[idx, :]\n",
    "                    L_train = self.laplace_matrix[idx][:,idx]\n",
    "                    # print(A_train.shape, L_train.shape)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.sdne(A_train, L_train)\n",
    "                    loss_epoch += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                if verbose > 0:\n",
    "                    print('Epoch {0}, loss {1} . >>> Epoch {2}/{3}'.format(epoch + 1, round(loss_epoch / num_samples, 4),\n",
    "                                                                         epoch + 1, epochs))\n",
    "    def get_embeddings(self):\n",
    "        if not self.embeddings:\n",
    "            self.__get_embeddings()\n",
    "        embeddings = self.embeddings\n",
    "        return embeddings\n",
    "\n",
    "    def __get_embeddings(self):\n",
    "        embeddings = {}\n",
    "        with torch.no_grad():\n",
    "            self.sdne.eval()\n",
    "            embed = self.sdne.encoder(self.adjacency_matrix)\n",
    "            for i, embedding in enumerate(embed.numpy()):\n",
    "                embeddings[self.idx2node[i]] = embedding\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __create_adjacency_laplace_matrix(self):\n",
    "        node_size = self.node_size\n",
    "        node2idx = self.node2idx\n",
    "        adjacency_matrix_data = []\n",
    "        adjacency_matrix_row_index = []\n",
    "        adjacency_matrix_col_index = []\n",
    "        for edge in self.graph.edges():\n",
    "            v1, v2 = edge\n",
    "            edge_weight = self.graph[v1][v2].get(\"weight\", 1.0)\n",
    "            adjacency_matrix_data.append(edge_weight)\n",
    "            adjacency_matrix_row_index.append(node2idx[v1])\n",
    "            adjacency_matrix_col_index.append(node2idx[v2])\n",
    "        adjacency_matrix = sparse.csr_matrix((adjacency_matrix_data,\n",
    "                                              (adjacency_matrix_row_index, adjacency_matrix_col_index)),\n",
    "                                             shape=(node_size, node_size))\n",
    "        adjacency_matrix_ = sparse.csr_matrix((adjacency_matrix_data+adjacency_matrix_data,\n",
    "                                               (adjacency_matrix_row_index+adjacency_matrix_col_index,\n",
    "                                                adjacency_matrix_col_index+adjacency_matrix_row_index)),\n",
    "                                              shape=(node_size, node_size))\n",
    "        degree_matrix = sparse.diags(adjacency_matrix_.sum(axis=1).flatten().tolist()[0])\n",
    "        laplace_matrix = degree_matrix - adjacency_matrix_\n",
    "        return adjacency_matrix, laplace_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdne import SDNE\n",
    "from basemodel import MultiClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## regulations weight ##########\n",
      "encoder.0.weight\n",
      "encoder.2.weight\n",
      "decoder.0.weight\n",
      "decoder.2.weight\n",
      "#########################\n",
      "batch_size(90) > node_size(90),set batch_size = 90\n",
      "Epoch 1, loss 0.2915 . >>> Epoch 1/20\n",
      "Epoch 2, loss 0.2868 . >>> Epoch 2/20\n",
      "Epoch 3, loss 0.2822 . >>> Epoch 3/20\n",
      "Epoch 4, loss 0.2773 . >>> Epoch 4/20\n",
      "Epoch 5, loss 0.2719 . >>> Epoch 5/20\n",
      "Epoch 6, loss 0.2658 . >>> Epoch 6/20\n",
      "Epoch 7, loss 0.2588 . >>> Epoch 7/20\n",
      "Epoch 8, loss 0.251 . >>> Epoch 8/20\n",
      "Epoch 9, loss 0.2424 . >>> Epoch 9/20\n",
      "Epoch 10, loss 0.2335 . >>> Epoch 10/20\n",
      "Epoch 11, loss 0.2245 . >>> Epoch 11/20\n",
      "Epoch 12, loss 0.2165 . >>> Epoch 12/20\n",
      "Epoch 13, loss 0.2105 . >>> Epoch 13/20\n",
      "Epoch 14, loss 0.207 . >>> Epoch 14/20\n",
      "Epoch 15, loss 0.2051 . >>> Epoch 15/20\n",
      "Epoch 16, loss 0.2031 . >>> Epoch 16/20\n",
      "Epoch 17, loss 0.1996 . >>> Epoch 17/20\n",
      "Epoch 18, loss 0.1946 . >>> Epoch 18/20\n",
      "Epoch 19, loss 0.1896 . >>> Epoch 19/20\n",
      "Epoch 20, loss 0.1855 . >>> Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "def read_node_label(file_path, skip_head=False):\n",
    "    X, y = [], []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        if skip_head:\n",
    "            f.readline()\n",
    "        for line in f.readlines():\n",
    "            tmp = line.strip().split(\" \")\n",
    "            X.append(tmp[0])\n",
    "            y.append(tmp[1:])\n",
    "    return X, y\n",
    "\n",
    "def plot_embeddings(embeddings, X, y):\n",
    "    embed_list = []\n",
    "    for node in X:\n",
    "        embed_list.append(embeddings[node])\n",
    "    tsne = TSNE(n_components=2)\n",
    "    node_tsned = tsne.fit_transform(np.asarray(embed_list), y)\n",
    "    color_idx = {}\n",
    "    for i in range(len(X)):\n",
    "        color_idx.setdefault(y[i][0], [])\n",
    "        color_idx[y[i][0]].append(i)\n",
    "    for c, idx in color_idx.items():\n",
    "        plt.scatter(node_tsned[idx, 0], node_tsned[idx, 1], label=c)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import networkx as nx\n",
    "    G = nx.read_edgelist('C:\\\\Users\\\\chaehyeon\\\\Desktop\\\\Wiki_edgelist.txt',\n",
    "                         create_using=nx.DiGraph(), nodetype=None, data=[('weight', int)])\n",
    "\n",
    "    model = SDNE(G, hidden_layers=[256, 128])\n",
    "    model.fit(batch_size=1024, epochs=20)\n",
    "    embeddings = model.get_embeddings()\n",
    "\n",
    "    X, y = read_node_label('C:\\\\Users\\\\chaehyeon\\\\Desktop\\\\wiki_labels.txt')\n",
    "    \n",
    "    model = MultiClassifier(embeddings, LogisticRegression())\n",
    "\n",
    "    '''model.evaluate_hold_out(X, y)\n",
    "\n",
    "    plot_embeddings(embeddings, X, y)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNN(nn.Module):\n",
    "    def __init__(self, node_size, nhid0, nhid1, droput, alpha):\n",
    "        super(MNN, self).__init__()\n",
    "        self.encode0 = nn.Linear(node_size, nhid0)\n",
    "        self.encode1 = nn.Linear(nhid0, nhid1)\n",
    "        self.decode0 = nn.Linear(nhid1, nhid0)\n",
    "        self.decode1 = nn.Linear(nhid0, node_size)\n",
    "        self.droput = droput\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, adj_batch, adj_mat, b_mat):\n",
    "        t0 = F.leaky_relu(self.encode0(adj_batch))\n",
    "        t0 = F.leaky_relu(self.encode1(t0))\n",
    "        embedding = t0\n",
    "        t0 = F.leaky_relu(self.decode0(t0))\n",
    "        t0 = F.leaky_relu(self.decode1(t0))\n",
    "        embedding_norm = torch.sum(embedding * embedding, dim=1, keepdim=True)\n",
    "        L_1st = torch.sum(adj_mat * (embedding_norm -\n",
    "                                     2 * torch.mm(embedding, torch.transpose(embedding, dim0=0, dim1=1))\n",
    "                                     + torch.transpose(embedding_norm, dim0=0, dim1=1)))\n",
    "        L_2nd = torch.sum(((adj_batch - t0) * b_mat) * ((adj_batch - t0) * b_mat))\n",
    "        return L_1st, self.alpha * L_2nd, L_1st + self.alpha * L_2nd\n",
    "\n",
    "    def savector(self, adj):\n",
    "        t0 = self.encode0(adj)\n",
    "        t0 = self.encode1(t0)\n",
    "        return t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-6118caf0644d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "from data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "./karate/karate.edgelist not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-849f60468772>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRead_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./karate/karate.edgelist'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0mData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-849f60468772>\u001b[0m in \u001b[0;36mRead_graph\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mRead_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0medge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mmin_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0medge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmin_node\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    622\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ./karate/karate.edgelist not found."
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def Read_graph(file_name):\n",
    "    edge = np.loadtxt(file_name).astype(np.int32)\n",
    "    min_node, max_node = edge.min(), edge.max()\n",
    "    if min_node == 0:\n",
    "        Node = max_node + 1\n",
    "    else:\n",
    "        Node = max_node\n",
    "    G = nx.Graph()\n",
    "    Adj = np.zeros([Node, Node], dtype=np.int32)\n",
    "    for i in range(edge.shape[0]):\n",
    "        G.add_edge(edge[i][0], edge[i][1])\n",
    "        if min_node == 0:\n",
    "            Adj[edge[i][0], edge[i][1]] = 1\n",
    "            Adj[edge[i][1], edge[i][0]] = 1\n",
    "        else:\n",
    "            Adj[edge[i][0] - 1, edge[i][1] - 1] = 1\n",
    "            Adj[edge[i][1] - 1, edge[i][0] - 1] = 1\n",
    "    Adj = torch.FloatTensor(Adj)\n",
    "    return G, Adj, Node\n",
    "\n",
    "class Dataload(data.Dataset):\n",
    "\n",
    "    def __init__(self, Adj, Node):\n",
    "        self.Adj = Adj\n",
    "        self.Node = Node\n",
    "    def __getitem__(self, index):\n",
    "        return index\n",
    "        # adj_batch = self.Adj[index]\n",
    "        # adj_mat = adj_batch[index]\n",
    "        # b_mat = torch.ones_like(adj_batch)\n",
    "        # b_mat[adj_batch != 0] = self.Beta\n",
    "        # return adj_batch, adj_mat, b_mat\n",
    "    def __len__(self):\n",
    "        return self.Node\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    G, Adj, Node = Read_graph('./karate/karate.edgelist')\n",
    "    Data = Dataload(Adj, Node)\n",
    "    Test = DataLoader(Data, batch_size=20, shuffle=True)\n",
    "    for index in Test:\n",
    "        adj_batch = Adj[index]\n",
    "        adj_mat = adj_batch[:, index]\n",
    "        b_mat = torch.ones_like(adj_batch)\n",
    "        b_mat[adj_batch != 0] = 5\n",
    "        print(b_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
